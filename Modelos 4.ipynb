{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e1d7d7-266a-4950-9357-8e19611a5a86",
   "metadata": {},
   "source": [
    "# Modelos 4: Tentando algo diferente\n",
    "**Nomes**: Bruna Guedes Pereira, Laura Medeiros Dal Ponte, Mariana Melo Pereira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35829ffe-d391-4d1f-b008-4b102aeecb5c",
   "metadata": {},
   "source": [
    "## Introdu√ß√£o\n",
    "\n",
    "O aprendizado de m√°quina consolidou-se como uma das ferramentas mais influentes da ci√™ncia contempor√¢nea, permitindo que dados complexos promovam decis√µes automatizadas. Dentro desse campo, os algoritmos de classifica√ß√£o ocupam papel central, pois possibilitam identificar padr√µes e organizar informa√ß√µes em categorias de forma precisa e eficiente.\n",
    "\n",
    "M√©todos cl√°ssicos como Support Vector Machine (SVM), Naive Bayes e Discriminant Analysis possuem relev√¢ncia hist√≥rica na resolu√ß√£o de problemas de classifica√ß√£o, sendo ainda muito utilizados em diversos experimentos cient√≠ficos. Explorar o funcionamento interno desses algoritmos, compreendendo suas bases matem√°ticas e avaliando seu desempenho em conjuntos de dados reais, √© um exerc√≠cio essencial de forma√ß√£o acad√™mica.\n",
    "\n",
    "Dessa forma, o presente trabalho dedica-se a investigar o algoritmo Naive Bayes, implement√°-lo de maneira did√°tica e comparar seus resultados a um modelo de refer√™ncia. Trata-se de uma fam√≠lia diversa de algoritmos, sendo dividida em tr√™s tipos: Gaussian Naive Bayes; Multinomial Naive Bayes e Bernoulli Naive Bayes. Aqui, focaremos apenas no Gaussian Naive Bayes, discutindo sua l√≥gica e seu desempenho.\n",
    "\n",
    "\n",
    "A seguir, uma breve contextualiza√ß√£o \"divertida\" para ambientar e motivar os leitores a entender os fundamentos da an√°lise realizada.\n",
    "\n",
    "### üéº **Ricardo e a Harmonia das Fronteiras: A Balada de Bayes**\n",
    "\n",
    "Ricardo nasceu em uma linhagem de cavaleiros honrados, treinado desde cedo para empunhar espadas e proteger reinos. Mas seu cora√ß√£o sempre bateu em outro ritmo ‚Äî o das melodias. Inspirado pelas apresenta√ß√µes da Senhorita Creizuda e da Alfaiate Ligeirinha, ele sonhava em ser um **bardo**, em transformar sentimentos em can√ß√µes e tocar almas com acordes sinceros.\n",
    "\n",
    "Mesmo preso √†s expectativas de sua fam√≠lia, Ricardo nunca abandonou sua paix√£o. Nas noites calmas, ele se refugia na **Floresta Solit√°ria**, onde canta sobre um amor que nunca desiste, nunca desilude, nunca fere. Suas can√ß√µes ecoam como promessas √† vida que deseja, mas que n√£o pode viver por completo.\n",
    "\n",
    "Foi em uma dessas noites que Ricardo encontrou um antigo manuscrito escondido sob uma raiz de √°rvore. Nele, estava descrito um algoritmo ancestral, usado por magos anal√≠ticos para **decifrar padr√µes ocultos com surpreendente simplicidade**: o **Naive Bayes**.\n",
    "\n",
    "Ricardo aprendeu que o Naive Bayes √© como um contador de hist√≥rias que confia em probabilidades para prever o rumo dos acontecimentos. Ele parte de uma suposi√ß√£o ousada ‚Äî que cada caracter√≠stica de um dado √© **independente das demais**, como notas que soam separadas, mas que juntas formam uma melodia coerente.\n",
    "\n",
    "A cada nova observa√ß√£o, o algoritmo calcula a **probabilidade de pertencimento a cada classe**, como se perguntasse: ‚ÄúQual √© a chance de esta can√ß√£o pertencer ao repert√≥rio da esperan√ßa, ou ao da saudade?‚Äù E mesmo com sua simplicidade, o Naive Bayes surpreende pela efici√™ncia, especialmente quando os dados s√£o vastos e as palavras s√£o muitas ‚Äî como em cartas, poemas ou can√ß√µes.\n",
    "\n",
    "Ricardo treinou um modelo Naive Bayes com dados reais, comparando seu desempenho com o de um classificador mais ing√™nuo. E descobriu que, como em suas baladas, **a beleza pode estar na simplicidade**, e que mesmo suposi√ß√µes aparentemente ing√™nuas podem revelar verdades profundas.\n",
    "\n",
    "Mesmo que nunca possa ser um bardo aos olhos do mundo, Ricardo sabe que, com cada linha de c√≥digo e cada nota tocada ao luar, ele est√° escrevendo sua pr√≥pria balada ‚Äî uma que fala de coragem, de aprendizado, e de nunca desistir daquilo que faz o cora√ß√£o vibrar.\n",
    "\n",
    "### O algoritmo Gaussian Naive Bayes\n",
    "\n",
    "O classificador Naive Bayes (NB) prev√™ dados baseando-se no Teorema de Bayes. Ele calcula a *probabilidade a posteriori* de cada uma das classes dado os features escolhidos, atribuindo os dados a classe (r√≥tulo) com maior *probabilidade a posteriori*. O Teorema de Bayes √© uma f√≥rmula da probabilidade condicional que considere a probabilidade de um evento $A$ ocorrer dado que um evento $B$ ocorreu. Ela √© escrita da seguinte forma: \n",
    "$$\n",
    "P(A | B) = \\frac{P(A) P(B | A).}{P(B)}.\n",
    "$$\n",
    "Onde $P(A)$ √© a probabilidade inicial; $P(B|A)$ √© a verossimilhan√ßa (probabilidade de $A$ dado que $B$ ocorreu); $P(B)$ √© a probabilidade individual (tamb√©m chamada de evid√™ncia); e $P(A|B)$ √© a probabilidade a posteriori.\n",
    "\n",
    "\"Naive\" quer dizer literalmente \"Ing√™nuo\", devido a l√≥gica simplista do algoritmo ao assumir que todas as features s√£o condicionalmente independentes umas das outras. Desse modo, cada observa√ß√£o serve como uma evid√™ncia independente no c√°lculo da probabilidade final de uma classe. Por exemplo: ele pode considerar que duas probabilidades s√£o independentes uma da outra, quando na verdade elas podem estar correlacionadas. Apesar disso, essa simplifica√ß√£o resulta em bons resultados. Ele tamb√©m atribui √°s features num√©ricas uma Distribui√ß√£o Gaussiana (ou Distribui√ß√£o Normal), que sera melhor evidenciada mais adiante. \n",
    "\n",
    "## üßë‚Äçüíª Vamos, ent√£o, para o C√≥digo M√°gico?\n",
    "\n",
    "O C√≥digo M√°gico, capaz de resolver o desafio proposto, ser√° constru√≠do a partir da linguagem Python, em conjunto com as suas bibliotecas `scikit-learn`, `pandas`, `scipy` e `statistics`. O primeiro passo ser√°, ent√£o, importar essas bibliotecas para o nosso ambiente de programa√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f3781",
   "metadata": {},
   "source": [
    "### Importa√ß√£o das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266dd07f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import norm\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd417e24",
   "metadata": {},
   "source": [
    "Como pr√©-requisito, o Naive Bayes √© projetado para trabalhar com valores cont√≠nuos e distribu√≠dos normalmente. Tamb√©m devemos checar se as features de nosso DataSet s√£o mesmo independentes umas das outras. Para essa demonstra√ß√£o, vamos utilizar o DataSet did√°tico `IRIS`. Ele √© um conjunto de dados simples, com quatro dados cont√≠nuos (largura e comprimento da s√©pala e largura e comprimento da p√©tala) e um dado categ√≥rico (nome da esp√©cie). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497cd81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IRIS.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fc332",
   "metadata": {},
   "source": [
    "A primeira coisa que o algoritmo faz √© calcular as m√©dias ($Œº$) e os desvios padr√µes ($œÉ$) de cada feature dentro das tr√™s classes. Abaixo, temos uma fun√ß√£o que demonstra de forma mais visual como isso funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300039c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def dados(df, coluna_classe):\n",
    "\n",
    "    groupby = df.groupby(coluna_classe)\n",
    "    media = groupby.mean()\n",
    "    dsvp = groupby.std()\n",
    "\n",
    "    return media, dsvp\n",
    "\n",
    "dados(df, \"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec4c99",
   "metadata": {},
   "source": [
    "Como consta no nome do algoritmo, tamb√©m podemos atribuir uma **curva gaussiana** que representa a distribui√ß√£o gaussiana de cada um dos features. O algoritmo usa uma fun√ß√£o de densidade de probabilidade (PDF), que √© dada por: \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\; e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Tendo isso em mente, podemos comparar as curvas gaussianas que obtemos de cada m√©dia e desvio padr√£o resultante dos tributos. Abaixo, temos uma visualiza√ß√£o do gr√°fico com as distribui√ß√µes gaussianas referentes ao comprimento da p√©tala de cada uma das tr√™s esp√©cies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4bd8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "media1 = 1.464 \n",
    "media2 = 5.552\n",
    "media3 = 4.260\n",
    "desvio1 = 0.173511\n",
    "desvio2 = 0.551895 \n",
    "desvio3 = 0.469911\n",
    "\n",
    "\n",
    "x =  np.linspace(media1 - 4*desvio1, media2 + 4*desvio2)\n",
    "\n",
    "y1 = norm.pdf(x, media1, desvio1)\n",
    "y2 = norm.pdf(x, media2, desvio2)\n",
    "y3 = norm.pdf(x, media3, desvio3)\n",
    "\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.plot(x, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c7043",
   "metadata": {},
   "source": [
    "De forma simples, o algoritmo armazena esses dados para obter as curvas gaussianas. Ao deparar-se com dados novos, o mesmo aplica o Teorema de Bayes para atribuir r√≥tulos. A partir do nosso dataset, podemos atribuir a seguinte l√≥gica:\n",
    "\n",
    "- P(A|B) √© a probabilidade de que uma flor seja de uma esp√©cie A, dado que as medidas de s√©pala e p√©tala coincidem com uma outra esp√©cie B.\n",
    "- P(A) √© a probabilidade inicial de que essa flor seja da esp√©cie A (sem nenhuma medida)\n",
    "- P(B|A) √© a verossimilhan√ßa; a probabilidade de que a flor tenha medidas de outra esp√©cie B dado que ela j√° √© uma flor da esp√©cie A.\n",
    "- P(B) √© a probabilidade de que essas medidas estejam presentes em outras esp√©cies de flor \n",
    "\n",
    "A probabilidade inicial parte de dados do pr√≥prio dataset; por exemplo, considerando que temos, ao todo, 150 flores para 3 esp√©cies, e que cada esp√©cie possui uma distribui√ß√£o igual, cada uma possu√≠ probabilidade inicial correspondente a ~33%. A partir disso, o algoritmo calcula a verossimilhan√ßa (P(A|B)). Devido a suposi√ß√£o *Naive*, ele a calcula multiplicando as quatro probabilidades iniciais. Por fim, ele utiliza o Teorema de Bayes para juntar a verossimilhan√ßa com a probabilidade inicial (que cont√™m nossa suposi√ß√£o da esp√©cie). A maior probabilidade a posteriori √©, portanto, considerada.\n",
    "\n",
    "### Exemplo de classifica√ß√£o\n",
    "\n",
    "A partir dos dados em nosso DataFrame, vamos prever uma certa amostra de dados, considerando a l√≥gica mencionada acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc0d02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop([\"species\"], axis=1)\n",
    "y = df[\"species\"]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_previsto = gnb.predict(X_test)\n",
    "\n",
    "y_previsto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb44f3",
   "metadata": {},
   "source": [
    "### Exemplo com um dado novo\n",
    "\n",
    "Supondo que obtemos informa√ß√µes sobre uma nova flor misteriosa, como ser√° que o nosso algoritmo se comporta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddc97d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop([\"species\"], axis=1)\n",
    "y = df[\"species\"]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "novos_valores = [6.1, 3.0, 4.9, 1.8]\n",
    "\n",
    "amostra_nova = pd.DataFrame([novos_valores], columns=X.columns)\n",
    "\n",
    "y_previsto = gnb.predict(X_test)\n",
    "\n",
    "y_previsto[0] ## Como ele retorna um array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565d62c",
   "metadata": {},
   "source": [
    "### A wild baseline appears!\n",
    "\n",
    "Agora que conseguimos prever nossos dados com o classificador Gaussian NB, vamos realizar uma previs√£o com um algoritmo baseline e comparar seu desempenho com o nosso algoritmo. O algoritmo K-NN √© um classificador baseline que prev√™ dados baseados no princ√≠pio de que semelhantes encontram-se pr√≥ximos, tomando como par√¢metros um n√∫mero K de vizinhos (a quantidade de dados pr√≥ximos que consideramos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec612b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df[[\"sepal_length\",\"sepal_width\",\"petal_length\"]] #Features\n",
    "y = df[[\"species\"]] #Target\n",
    "\n",
    "## 1) Instanciar o modelo usando o algoritmo K-NN com o n√∫mero de vizinhos desejados\n",
    "modelo_knn = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "## 2) Separar os dados em teste e treino\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "## 3) Treinar os dados\n",
    "modelo_knn.fit(X_treino, y_treino)\n",
    "\n",
    "## 4) Avalia√ß√£o do modelo\n",
    "previsao_y = modelo_knn.predict(X_teste)\n",
    "previsao_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a65ed0",
   "metadata": {},
   "source": [
    "### Computando o desempenho\n",
    "\n",
    "Para fins de curiosidade, vamos comparar o desempenho entre esses dois algoritmos. A fun√ß√£o `acuraccy_score` do `scikit-learn` calcula a acur√°cia das previs√µes do algoritmo, ou seja, qu√£o eficaz ele √© em prever y que s√£o exatamente o y verdadeiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d987152",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "acuracia_knn = accuracy_score(y_teste, y_previsto)\n",
    "acuracia_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b324db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "acuracia = accuracy_score(y_test, y_previsto)\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47d434-5af3-4e19-bb20-35e183936999",
   "metadata": {},
   "source": [
    "## Resultados e Discuss√µes\n",
    "\n",
    "Ao aplicar o algoritmo Naive Bayes em um conjunto de dados real, foi poss√≠vel observar seu desempenho em compara√ß√£o com um classificador baseline, revelando aspectos importantes sobre sua capacidade de generaliza√ß√£o e sensibilidade √†s caracter√≠sticas dos dados. Os resultados mostraram que, embora o modelo baseline ofere√ßa uma refer√™ncia √∫til, o algoritmo estudado apresentou maior precis√£o e melhor separa√ß√£o entre as classes, especialmente em cen√°rios com dados bem distribu√≠dos. Essa an√°lise refor√ßa a import√¢ncia de entender o funcionamento interno dos algoritmos para fazer escolhas mais informadas em projetos de aprendizado de m√°quina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297e6b7",
   "metadata": {},
   "source": [
    "## Conclus√µes\n",
    "\n",
    "Este notebook teve como objetivo compreender e aplicar de forma did√°tica um algoritmo de classifica√ß√£o supervisionada, destacando sua fundamenta√ß√£o te√≥rica. Ao induzir um modelo com o algoritmo escolhido e compar√°-lo com um classificador baseline, foi poss√≠vel avaliar seu desempenho e entender suas vantagens e limita√ß√µes em cen√°rios reais. Percebe-se que, ao comparar o desempenho do algoritmo Gaussian Naive Bayes com o desempenho de um algoritmo baseline como o K-NN, obteu-se que a acur√°cia referente ao nosso Gaussian NB √© significantemente maior do que a do KNN. Apesar da l√≥gica simplificada do Gaussian Naive Bayes, ele demonstra-se muito eficiente em realizar previs√µes bastante pr√≥ximas a realidade, o que o diferencia de um algoritmo categ√≥rico de baseline. Assim, o material contribui para o desenvolvimento de uma habilidade essencial: a capacidade de estudar e aplicar algoritmos n√£o abordados diretamente em sala de aula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21562de-ace4-4012-94d2-69fda8353026",
   "metadata": {},
   "source": [
    "## Refer√™ncias Bibliogr√°ficas\n",
    "1. IBM. M√°quina de vetores de suporte. Dispon√≠vel em: <https://www.ibm.com/br-pt/think/topics/support-vector-machine>.\n",
    "2. OPENAI. ChatGPT. S√£o Francisco: OpenAI, 2025. Dispon√≠vel em: <https://chat.openai.com/>. Acesso em: 10 set. 2025.\n",
    "3. WHYAMIT404. Creating a Gaussian Distribution with NumPy. Dispon√≠vel em: <https://medium.com/@whyamit404/creating-a-gaussian-distribution-with-numpy-dc810d931ee0>. Acesso em: 29 set. 2025.\n",
    "4. RYAN & MATT DATA SCIENCE. Naive Bayes Classifier: A Practical Tutorial with Scikit-Learn. Dispon√≠vel em: <https://www.youtube.com/watch?v=T6OxPncD7_w>.\n",
    "5. RYAN & MATT DATA SCIENCE. Normal Distribution in Python: A Beginner‚Äôs Guide with Scipy & Numpy. Dispon√≠vel em: <https://www.youtube.com/watch?v=_Q-CAxnd69U>. Acesso em: 29 set. 2025.\n",
    "6. Fit Normal Distribution Object. Dispon√≠vel em: <https://www.mathworks.com/help/stats/normal-distribution.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4920f98",
   "metadata": {},
   "source": [
    "## Uso de Intelig√™ncia Artificial\n",
    "\n",
    "O modelo Microsoft Copilot foi utilizado especificamente para criar a hist√≥ria inicial no estilo RPG e para revisar os textos de desenvolvimento dos exerc√≠cios, ajudando a detalhar o passo a passo seguido para guiar melhor o leitor. As perguntas realizadas √† IA foram no estilo: *\"Escrevi o seguinte texto: ... Como posso melhor√°-lo para guiar o leitor passo a passo, mostrando de maneira clara minha resolu√ß√£o?\"*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
